#!/bin/sh -e

##########################################################################
#   Description:
#       Run kallisto quantification for each RNA sample.
#
#       All necessary tools are assumed to be in PATH.  If this is not
#       the case, add whatever code is needed here to gain access.
#       (Adding such code to your .bashrc or other startup script is
#       generally a bad idea since it's too complicated to support
#       every program with one environment.)
#       
#   History:
#   Date        Name        Modification
#   2019-11-??  Jason Bacon Begin
##########################################################################

#lpjs jobs 15
#lpjs processors-per-job 4
#lpjs threads-per-process processors-per-job
# Generated by lpjs peak-mem 09-kallisto-quant.lpjs
#lpjs pmem-per-processor 229MiB

##############################################################################
# Run kallisto with 500 bootstraps for Sleuth
#
# --genomebam is needed to generate a genome-mapped BAM file for browsing with
# IGV.  It requires --gtf and --chromosomes. --chromosomes requires a TSV file
# with chromosome name and length on each line.  The chromosome names in the
# TSV must exactly match the names in the GTF.
# https://github.com/pachterlab/kallisto/issues/155
#
# The format and source of the chromosomes TSV is not clearly documented.
# I generated one using an Ensemble GFF with Reference/create-chrom-sizes.sh.
# GTF does not contain chromosome features.

# Set a default value for testing outside the SLURM environment
: ${LPJS_ARRAY_INDEX:=1}
sample=$LPJS_ARRAY_INDEX

# Document software versions used for publication
uname -a
kallisto version
pwd

gtf=$(Reference/gtf-filename.sh)

# If using hdf5, you may need this:
# https://github.com/pachterlab/kallisto/issues/197
# export HDF5_USE_FILE_LOCKING=FALSE

# 6-merge-bams.sbatch relies on sample N being in Results/09-kallisto-quant/N
# The sample number comes after -sample in the filename, e.g.
# chondro-sample4-rep2-time1-R1.fastq.xz is sample 4

# kallisto can't handle zstd and will simply seg fault rather than
# issue an error message.  Manually decompress the zstd files into
# a named pipe and let kallisto read from there.
input_dir=Results/04-trim
output_dir=Results/09-kallisto-quant
zst1=$(echo $input_dir/*sample${sample}-*-R1.fastq.zst)
zst2=$(echo $input_dir/*sample${sample}-*-R2.fastq.zst)
pipe1=/tmp/$(basename ${zst1%.zst}).fifo
pipe2=/tmp/$(basename ${zst2%.zst}).fifo
mkfifo $pipe1 $pipe2 || true
zstdcat $zst1 > $pipe1 &
zstdcat $zst2 > $pipe2 &

# Kallisto requires an output subdirectory for each sample
stem=$(basename ${zst1%-R1*})
my_output_dir=$output_dir/$stem
mkdir -p $my_output_dir

# Manual says a GTF is needed.  Kallisto aborts using GFF3.
set -x
kallisto quant \
    --threads=${LPJS_THREADS_PER_PROCESS} \
    --index=Results/08-kallisto-index/all-but-xy.index \
    --output-dir=$my_output_dir $pipe1 $pipe2
rm -f $pipe1 $pipe2
